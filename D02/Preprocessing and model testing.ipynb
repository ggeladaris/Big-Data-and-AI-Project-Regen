{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f946ee1",
   "metadata": {},
   "source": [
    "### We  import the  libraries and read our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0f9d272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib \n",
    "from sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error,median_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error,median_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pprint import pprint\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "clf = linear_model.GammaRegressor(alpha=0)\n",
    "import ast\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('listings.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883fe9e",
   "metadata": {},
   "source": [
    "### We remove all the columns that we don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d77585f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['host_response_time','host_response_rate','host_acceptance_rate',\n",
    "        'reviews_per_month','calculated_host_listings_count_private_rooms','calculated_host_listings_count_shared_rooms',\n",
    "        'id','listing_url','scrape_id','last_scraped','name','description','neighborhood_overview',\n",
    "        'picture_url','host_id', 'host_url','host_name', 'host_since', 'host_location', 'host_about',\n",
    "        'host_thumbnail_url', 'host_picture_url','host_neighbourhood','neighbourhood','neighbourhood_group_cleansed',\n",
    "        'latitude','longitude','bathrooms','calendar_updated','calendar_last_scraped','first_review', 'last_review',\n",
    "        'license','host_total_listings_count','minimum_minimum_nights', 'maximum_minimum_nights',\n",
    "        'minimum_maximum_nights', 'maximum_maximum_nights','minimum_nights_avg_ntm',\n",
    "         'availability_60','number_of_reviews_ltm', 'calculated_host_listings_count',\n",
    "        'availability_90','property_type'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a574a",
   "metadata": {},
   "source": [
    "### We transform the price column from 'str' to 'float' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bb586424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price']=df['price'].str.strip('$')\n",
    "df['price'] = df['price'].apply(lambda x: float(x.split()[0].replace(',', '')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714add20",
   "metadata": {},
   "source": [
    "### We transform non-numerical labels  to numerical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6d11e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "le=LabelEncoder()\n",
    "df['host_is_superhost']=le.fit_transform(df['host_is_superhost'])\n",
    "df['host_has_profile_pic']=le.fit_transform(df['host_has_profile_pic'])\n",
    "df['host_identity_verified']=le.fit_transform(df['host_identity_verified'])\n",
    "df['has_availability']=le.fit_transform(df['has_availability'])\n",
    "df['instant_bookable']=le.fit_transform(df['instant_bookable'])\n",
    "df['neighbourhood_cleansed']=le.fit_transform(df['neighbourhood_cleansed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "85741985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['0', '1', '2', '3'], dtype=object),\n",
       " array([  90,  891, 8463,  138], dtype=int64))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Entire home/apt'-3, 'Hotel room'-2, 'Private room'-1, 'Shared room'-0\n",
    "df['room_type'] = df['room_type'].map(lambda x: 0 if x=='Shared room' else (1 if x=='Private room' else(2 if x=='Entire home/apt' else 3)))\n",
    "np.unique(np.array(df['room_type'].astype('str')),return_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "46a6aa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0 baths': 0, '0 shared baths': 0, '1 bath': 6, '1 private bath': 5, '1 shared bath': 4, '1.5 baths': 8, '1.5 shared baths': 7, '12 baths': 27, '2 baths': 10, '2 shared baths': 9, '2.5 baths': 12, '2.5 shared baths': 11, '3 baths': 14, '3 shared baths': 13, '3.5 baths': 16, '3.5 shared baths': 15, '4 baths': 18, '4 shared baths': 17, '4.5 baths': 19, '5 baths': 21, '5 shared baths': 20, '5.5 baths': 22, '6 baths': 23, '6.5 baths': 24, '7 baths': 25, '9.5 baths': 26, 'Half-bath': 3, 'Shared half-bath': 2, 'nan': nan}\n"
     ]
    }
   ],
   "source": [
    "cat=np.unique(np.array(df['bathrooms_text'].astype('str')))[0:-1]\n",
    "cat[-1]='0.3 '+cat[-1]\n",
    "cat[-2]='0.5 '+cat[-2]\n",
    "dict={}\n",
    "\n",
    "for i,elem in enumerate(cat):\n",
    "    if 'shared' in elem:\n",
    "        dict[elem]=float(''.join(i for i in elem if (i.isdigit() or i == \".\")))-0.2\n",
    "    elif 'private' in elem:\n",
    "        dict[elem]=float(''.join(i for i in elem if (i.isdigit() or i == \".\")))-0.1\n",
    "    else:\n",
    "        dict[elem]=float(''.join(i for i in elem if (i.isdigit() or i == \".\")))\n",
    "    \n",
    "a = sorted(dict.items(), key=lambda x: x[1])\n",
    "j=0\n",
    "for i in a:\n",
    "    dict[i[0]]=j\n",
    "    j+=1\n",
    "\n",
    "dict['0 baths']=0\n",
    "dict['Half-bath'] = dict['0.5 Half-bath']\n",
    "dict['Shared half-bath']=dict['0.3 Shared half-bath']\n",
    "del dict['0.3 Shared half-bath']\n",
    "del dict['0.5 Half-bath']\n",
    "dict['nan']=np.NaN\n",
    "print(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4c6440d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bathrooms_text'].replace(dict,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "95506a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['host_verifications'] = df['host_verifications'].apply(lambda x: ast.literal_eval(x))\n",
    "df['host_verifications'] = df['host_verifications'].fillna(value=np.nan)\n",
    "for y in range(9582):\n",
    "    if type(df['host_verifications'][y])==list:\n",
    "        df.at[y,'host_verifications'] = len(df['host_verifications'][y])\n",
    "\n",
    "df['amenities'] = df['amenities'].apply(lambda x: ast.literal_eval(x))\n",
    "for y in range(9582):\n",
    "    if type(df['amenities'][y])==list:\n",
    "        df.at[y,'amenities'] = len(df['amenities'][y])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "46eb7fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<class 'int'>], dtype=object)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['amenities'].apply(type).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "75348795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39b7ce",
   "metadata": {},
   "source": [
    "### We use one hot encoding to handle the neghbourhood_neansed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8e0ab9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, prefix=['neighbourhood_cleansed'], columns=['neighbourhood_cleansed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423ab932",
   "metadata": {},
   "source": [
    "### We remove the price column and split our dataset into a train set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1d60557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_1=df.copy()\n",
    "y=df_1['price']\n",
    "df_1.drop(labels='price',axis=1,inplace=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_1, y, test_size=0.40,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0235e4",
   "metadata": {},
   "source": [
    "### We handle nan values remove Ocertain outliers and normalize our tsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "089b40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train[y_train<800]\n",
    "y_train=y_train[y_train<800]\n",
    "y_train=np.log(y_train)\n",
    "y_test=np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "06a749a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "imp = IterativeImputer(max_iter=20, random_state=0)\n",
    "imp.fit(x_train)\n",
    "x_train= imp.transform(x_train)\n",
    "x_test= imp.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4aba91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(x_train)\n",
    "x_train=scaler.transform(x_train)\n",
    "x_test=scaler.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364d183",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1525e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error= 0.3090522538548155\n",
      "Median Absolute Error= 11.475271349724771\n",
      "Mean Absolute Error= 36.447877109398156\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "predictions=model.predict(x_test)\n",
    "\n",
    "y_test_=np.exp(y_test)\n",
    "predictions_=np.exp(predictions)\n",
    "print(f'Mean Absolute Percentage Error= {mean_absolute_percentage_error(y_test_, predictions_)}')\n",
    "print(f'Median Absolute Error= {median_absolute_error(y_test_, predictions_)}')\n",
    "print(f'Mean Absolute Error= {mean_absolute_error(y_test_, predictions_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6a0af",
   "metadata": {},
   "source": [
    "#### Random Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "53c07035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'squared_error',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "pprint(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a06ae087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt', 'log2'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 500, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cfb45d",
   "metadata": {},
   "source": [
    "#### Random Search training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d3c02976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 100, \n",
    "                               cv = 10, \n",
    "                               verbose=2, \n",
    "                               random_state=42, \n",
    "                               n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7ee5670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the best parameters from fitting the random search\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e5379",
   "metadata": {},
   "source": [
    "### Random Forest after hyperparameters' tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "cf4b6a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error= 0.2891002467880968\n",
      "Median Absolute Error= 10.882625331644931\n",
      "Mean Absolute Error= 35.55695879949372\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# With the tuned hyperparameters from Random Search \n",
    "model = RandomForestRegressor(n_estimators = 400, min_samples_split = 2, min_samples_leaf = 1,\n",
    "                              max_features = 'log2', max_depth = None, bootstrap = False)\n",
    "model.fit(x_train, y_train)\n",
    "predictions=model.predict(x_test)\n",
    "\n",
    "y_test_=np.exp(y_test)\n",
    "predictions_=np.exp(predictions)\n",
    "print(f'Mean Absolute Percentage Error= {mean_absolute_percentage_error(y_test_, predictions_)}')\n",
    "print(f'Median Absolute Error= {median_absolute_error(y_test_, predictions_)}')\n",
    "print(f'Mean Absolute Error= {mean_absolute_error(y_test_, predictions_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925cb27",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "019123e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error = 0.37612652333486596\n",
      "Median Absolute Error = 13.811843341454555\n",
      "Mean Absolute Error = 40.48833140334374\n"
     ]
    }
   ],
   "source": [
    "# define the pipeline and train model\n",
    "model2 = Pipeline([('poly', PolynomialFeatures(degree=1)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "                  \n",
    "model2.fit(x_train, y_train)\n",
    "predictions2=model2.predict(x_test)\n",
    "\n",
    "y_test_ = np.exp(y_test)\n",
    "predictions_ = np.exp(predictions2)\n",
    "print(f'Mean Absolute Percentage Error = {mean_absolute_percentage_error(y_test_, predictions_)}')\n",
    "print(f'Median Absolute Error = {median_absolute_error(y_test_, predictions_)}')\n",
    "print(f'Mean Absolute Error = {mean_absolute_error(y_test_, predictions_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7857d6",
   "metadata": {},
   "source": [
    "### Robust Regression — RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3606b5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error = 0.37613265631658094\n",
      "Median Absolute Error = 13.814462103399997\n",
      "Mean Absolute Error = 40.48852482534842\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set RANSAC hyperparameters\n",
    "ransac = RANSACRegressor(LinearRegression(),\n",
    "\t\tmax_trials=50, \t\t# Number of Iterations\n",
    "\t\tmin_samples=20, \t\t# Minimum size of the sample\n",
    "\t\tloss='absolute_error', \t# Metrics for loss\n",
    "\t\tresidual_threshold=15 \t# Threshold\n",
    "\t\t)\n",
    "\n",
    "# Train model\n",
    "ransac.fit(x_train, y_train)\n",
    "predictions3=ransac.predict(x_test)\n",
    "\n",
    "y_test_ = np.exp(y_test)\n",
    "predictions_ = np.exp(predictions3)\n",
    "print(f'Mean Absolute Percentage Error = {mean_absolute_percentage_error(y_test_, predictions_)}')\n",
    "print(f'Median Absolute Error = {median_absolute_error(y_test_, predictions_)}')\n",
    "print(f'Mean Absolute Error = {mean_absolute_error(y_test_, predictions_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a27da",
   "metadata": {},
   "source": [
    "### GLM: Gamma link function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0c2fb66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error = 0.3789044515241129\n",
      "Median Absolute Error = 13.64477764536581\n",
      "Mean Absolute Error = 41.487611527964866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf.fit(x_train, y_train)\n",
    "predictions4=clf.predict(x_test)\n",
    "\n",
    "y_test_ = np.exp(y_test)\n",
    "predictions_ = np.exp(predictions4)\n",
    "print(f'Mean Absolute Percentage Error = {mean_absolute_percentage_error(y_test_, predictions_)}')\n",
    "print(f'Median Absolute Error = {median_absolute_error(y_test_, predictions_)}')\n",
    "print(f'Mean Absolute Error = {mean_absolute_error(y_test_, predictions_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d8348",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5ae1592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error = 0.3351595356152841\n",
      "Median Absolute Error = 11.934978401631845\n",
      "Mean Absolute Error = 38.785403463786245\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose regression method and set hyperparameter\n",
    "svr_rbf = SVR(C=1.0, epsilon=0.2, kernel='rbf')\n",
    "\n",
    "# Training of the regression model\n",
    "svr_rbf.fit(x_train, y_train)\n",
    "y_pred = svr_rbf.predict(x_test)\n",
    "\n",
    "y_test_ = np.exp(y_test)\n",
    "predictions_ = np.exp(y_pred)\n",
    "\n",
    "print(f'Mean Absolute Percentage Error = {mean_absolute_percentage_error(y_test_, predictions_)}')\n",
    "print(f'Median Absolute Error = {median_absolute_error(y_test_, predictions_)}')\n",
    "print(f'Mean Absolute Error = {mean_absolute_error(y_test_, predictions_)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
